# ğŸ”‘ é¡¹ç›®æ ¸å¿ƒæ–¹æ³•ä¸ä»£ç 

---

## 1. ç¥ç»ç½‘ç»œç»“æ„ (`dqn_network.py`)

**DQNNetwork** â€” å·ç§¯ç¥ç»ç½‘ç»œï¼Œå°†æ¸¸æˆå¸§æ˜ å°„ä¸ºå„åŠ¨ä½œçš„ Q å€¼ï¼š

```python
class DQNNetwork(Model):
    def __init__(self, num_actions, agent_history_length):
        self.normalize = Lambda(lambda x: x / 255.0)  # å½’ä¸€åŒ–
        self.conv1 = Conv2D(32, 8, strides=4, activation="relu", 
                           kernel_initializer=VarianceScaling(2.0))
        self.conv2 = Conv2D(64, 4, strides=2, activation="relu", ...)
        self.conv3 = Conv2D(64, 3, strides=1, activation="relu", ...)
        self.flatten = Flatten()
        self.dense1 = Dense(512, activation='relu', ...)
        self.dense2 = Dense(num_actions, activation="linear")  # è¾“å‡º Q(s,a)

    @tf.function
    def call(self, x):
        x = self.normalize(x)
        x = self.conv1(x) â†’ self.conv2(x) â†’ self.conv3(x)
        x = self.flatten(x) â†’ self.dense1(x) â†’ self.dense2(x)
        return x  # shape: (batch, num_actions)
```

---

## 2. Îµ-è´ªå¿ƒåŠ¨ä½œé€‰æ‹© (`get_action`)

```python
@tf.function
def get_action(self, state, exploration_rate):
    if tf.random.uniform(()) < exploration_rate:
        action = tf.random.uniform((), maxval=num_actions, dtype=tf.int32)  # æ¢ç´¢
    else:
        q_value = self.main_network(state)
        action = tf.argmax(q_value, axis=1)  # åˆ©ç”¨
    return action
```

---

## 3. DQN çš„ Q ç½‘ç»œæ›´æ–° (`dqn_agent.py`)

**å…³é”®**ï¼šç›´æ¥ç”¨ç›®æ ‡ç½‘ç»œçš„æœ€å¤§ Q å€¼

```python
@tf.function
def update_main_q_network(self, state_batch, action_batch, reward_batch, 
                          next_state_batch, terminal_batch):
    with tf.GradientTape() as tape:
        # DQN: ç”¨ target network çš„ max Q
        q_target = self.target_network(next_state_batch)
        max_q_target = tf.reduce_max(q_target, axis=1)  # â˜… DQN æ ¸å¿ƒ
        
        expected_q = reward + Î³ * max_q_target * (1 - done)
        main_q = self.main_network(state_batch)[action]
        loss = Huber(expected_q, main_q)
    
    gradients = tape.gradient(loss, self.main_network.trainable_variables)
    self.optimizer.apply_gradients(...)
```

---

## 4. DDQN çš„ Q ç½‘ç»œæ›´æ–° (`ddqn_agent.py`)

**å…³é”®**ï¼šç”¨ä¸»ç½‘ç»œé€‰åŠ¨ä½œï¼Œç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼° Q å€¼ï¼ˆè§£è€¦ï¼‰

```python
@tf.function
def update_main_q_network(self, state_batch, action_batch, reward_batch,
                          next_state_batch, terminal_batch):
    with tf.GradientTape() as tape:
        # DDQN: ä¸»ç½‘ç»œé€‰åŠ¨ä½œï¼Œç›®æ ‡ç½‘ç»œè¯„ä¼°
        q_online = self.main_network(next_state_batch)
        action_q_online = tf.argmax(q_online, axis=1)  # â˜… ä¸»ç½‘ç»œé€‰åŠ¨ä½œ
        
        q_target = self.target_network(next_state_batch)
        ddqn_q = q_target[action_q_online]  # â˜… ç›®æ ‡ç½‘ç»œè¯„ä¼°è¯¥åŠ¨ä½œ
        
        expected_q = reward + Î³ * ddqn_q * (1 - done)
        main_q = self.main_network(state_batch)[action]
        loss = Huber(expected_q, main_q)
    
    gradients = tape.gradient(loss, self.main_network.trainable_variables)
    self.optimizer.apply_gradients(...)
```

---

## 5. ç›®æ ‡ç½‘ç»œåŒæ­¥ (`update_target_network`)

```python
@tf.function
def update_target_network(self):
    for main_var, target_var in zip(
        self.main_network.trainable_variables,
        self.target_network.trainable_variables
    ):
        target_var.assign(main_var)  # ç¡¬æ›´æ–°
```

---

## 6. ç»éªŒå›æ”¾ (`memory.py`)

```python
Transition = namedtuple("Transition", ("state", "action", "reward", "next_state", "terminal"))

class ReplayMemory:
    def __init__(self, capacity=10000, minibatch_size=32):
        self._memory = []
        self._index = 0
    
    def push(self, state, action, reward, next_state, terminal):
        """å¾ªç¯è¦†ç›–å­˜å‚¨ç»éªŒ"""
        trsn = Transition(state, action, reward, next_state, terminal)
        self._memory[self._index] = trsn
        self._index = (self._index + 1) % self.capacity
    
    def get_minibatch_indices(self):
        """éšæœºé‡‡æ ·ç´¢å¼•ï¼ˆé¿å…è·¨ episode è¾¹ç•Œï¼‰"""
        indices = np.random.randint(history_len, len(self), size=batch_size)
        return indices
    
    def generate_minibatch_samples(self, indices):
        """æ ¹æ®ç´¢å¼•ç”Ÿæˆè®­ç»ƒæ‰¹æ¬¡"""
        return state_batch, action_batch, reward_batch, next_state_batch, terminal_batch
```

---

## 7. Îµ è¡°å‡ç­–ç•¥ (`get_eps`)

```python
@tf.function
def get_eps(self, current_step):
    # çº¿æ€§è¡°å‡: 1.0 â†’ 0.1 (å‰ 100 ä¸‡æ­¥)
    if current_step < replay_start_size:
        eps = 1.0
    elif current_step < final_explr_frame:
        eps = 1.0 - 0.9 * (current_step - replay_start_size) / (final_explr_frame - replay_start_size)
    else:
        eps = 0.1 â†’ 0.01 (ç»§ç»­è¡°å‡)
    return eps
```

---

## 8. è®­ç»ƒä¸»å¾ªç¯ (`train`)

```python
def train(self):
    while total_step < training_frames:
        state = env.reset()
        while not done:
            # 1. é€‰åŠ¨ä½œ
            action = get_action(state, get_eps(total_step))
            
            # 2. æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = env.step(action)
            
            # 3. å­˜ç»éªŒ
            memory.push(state, action, reward, next_state, done)
            
            # 4. æ¯ 4 æ­¥æ›´æ–°ä¸»ç½‘ç»œ
            if total_step % 4 == 0 and total_step > replay_start_size:
                batch = memory.sample()
                update_main_q_network(batch)
            
            # 5. æ¯ 1000 æ­¥åŒæ­¥ç›®æ ‡ç½‘ç»œ
            if total_step % 1000 == 0:
                update_target_network()
            
            state = next_state
```

---

## ğŸ“Š DQN vs DDQN æ ¸å¿ƒå·®å¼‚å¯¹æ¯”

| ç®—æ³• | TD ç›®æ ‡è®¡ç®— | ä»£ç å…³é”®è¡Œ |
|------|-------------|-----------|
| **DQN** | $r + \gamma \max_{a'} Q_{target}(s', a')$ | `tf.reduce_max(q_target, axis=1)` |
| **DDQN** | $r + \gamma Q_{target}(s', \arg\max_{a'} Q_{main}(s', a'))$ | `tf.argmax(q_online)` + `q_target[action]` |

---

ä»¥ä¸Šå°±æ˜¯æœ¬é¡¹ç›®æœ€é‡è¦çš„æ–¹æ³•å®ç°å’Œä»£ç ç‰‡æ®µï¼Œæ¶µç›–äº†ç½‘ç»œæ¶æ„ã€åŠ¨ä½œé€‰æ‹©ã€Q å€¼æ›´æ–°ï¼ˆDQN/DDQNï¼‰ã€ç»éªŒå›æ”¾å’Œè®­ç»ƒæµç¨‹çš„æ ¸å¿ƒé€»è¾‘ã€‚
![alt text](image.png)